{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669f65c4-ed25-4663-aa30-d0d155ac51e9",
   "metadata": {},
   "source": [
    "# The Second Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83619dcf-fe7f-422a-bd9f-49e5582d2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in packages\n",
    "import string\n",
    "import os\n",
    "import time as t\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Basic Packages for Data Wrangling\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import datetime as dt\n",
    "\n",
    "# NLTK for processing stop words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer # Great stemmer\n",
    "from nltk.stem import LancasterStemmer # over-stemming easily, more aggresive stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer # stemmer for non-english languages\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizer (词形还原)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # get tf-idf matrix\n",
    "from pyinflect import getAllInflections # get inflect words\n",
    "\n",
    "import re # For regular expression\n",
    "from tqdm import tqdm # Processing bar\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f89285d-fc49-4abc-9758-7b29fa8632ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "stop_words.extend( [ \"\",\n",
    "                    \"a\",\n",
    "                    \"able\",\n",
    "                    \"about\",\n",
    "                    \"above\",\n",
    "                    \"according\",\n",
    "                    \"accordingly\",\n",
    "                    \"across\",\n",
    "                    \"actually\",\n",
    "                    \"after\",\n",
    "                    \"afterwards\",\n",
    "                    \"again\",\n",
    "                    \"against\",\n",
    "                    \"all\",\n",
    "                    \"allow\",\n",
    "                    \"almost\",\n",
    "                    \"alone\",\n",
    "                    \"along\",\n",
    "                    \"already\",\n",
    "                    \"also\",\n",
    "                    \"although\",\n",
    "                    \"always\",\n",
    "                    \"am\",\n",
    "                    \"among\",\n",
    "                    \"amongst\",\n",
    "                    \"an\",\n",
    "                    \"and\",\n",
    "                    \"another\",\n",
    "                    \"any\",\n",
    "                    \"anybody\",\n",
    "                    \"anyhow\",\n",
    "                    \"anyone\",\n",
    "                    \"anything\",\n",
    "                    \"anyway\",\n",
    "                    \"anyways\",\n",
    "                    \"anywhere\",\n",
    "                    \"apart\",\n",
    "                    \"appear\",\n",
    "                    \"appreciate\",\n",
    "                    \"appropriate\",\n",
    "                    \"are\",\n",
    "                    \"around\",\n",
    "                    \"as\",\n",
    "                    \"aside\",\n",
    "                    \"ask\",\n",
    "                    \"asking\",\n",
    "                    \"associated\",\n",
    "                    \"at\",\n",
    "                    \"available\",\n",
    "                    \"away\",\n",
    "                    \"awfully\",\n",
    "                    \"b\",\n",
    "                    \"be\",\n",
    "                    \"became\",\n",
    "                    \"because\",\n",
    "                    \"become\",\n",
    "                    \"becomes\",\n",
    "                    \"becoming\",\n",
    "                    \"been\",\n",
    "                    \"before\",\n",
    "                    \"beforehand\",\n",
    "                    \"behind\",\n",
    "                    \"being\",\n",
    "                    \"believe\",\n",
    "                    \"below\",\n",
    "                    \"beside\",\n",
    "                    \"besides\",\n",
    "                    \"best\",\n",
    "                    \"better\",\n",
    "                    \"between\",\n",
    "                    \"beyond\",\n",
    "                    \"both\",\n",
    "                    \"brief\",\n",
    "                    \"but\",\n",
    "                    \"by\",\n",
    "                    \"c\",\n",
    "                    \"came\",\n",
    "                    \"can\",\n",
    "                    \"cannot\",\n",
    "                    \"cant\",\n",
    "                    \"cause\",\n",
    "                    \"causes\",\n",
    "                    \"certain\",\n",
    "                    \"certainly\",\n",
    "                    \"changes\",\n",
    "                    \"clearly\",\n",
    "                    \"co\",\n",
    "                    \"com\",\n",
    "                    \"come\",\n",
    "                    \"comes\",\n",
    "                    \"concerning\",\n",
    "                    \"consequently\",\n",
    "                    \"consider\",\n",
    "                    \"considering\",\n",
    "                    \"contain\",\n",
    "                    \"containing\",\n",
    "                    \"contains\",\n",
    "                    \"corresponding\",\n",
    "                    \"could\",\n",
    "                    \"course\",\n",
    "                    \"currently\",\n",
    "                    \"d\",\n",
    "                    \"definitely\",\n",
    "                    \"described\",\n",
    "                    \"despite\",\n",
    "                    \"did\",\n",
    "                    \"different\",\n",
    "                    \"do\",\n",
    "                    \"does\",\n",
    "                    \"doing\",\n",
    "                    \"done\",\n",
    "                    \"down\",\n",
    "                    \"downwards\",\n",
    "                    \"during\",\n",
    "                    \"dr\",\n",
    "                    \"e\",\n",
    "                    \"each\",\n",
    "                    \"edu\",\n",
    "                    \"eg\",\n",
    "                    \"eight\",\n",
    "                    \"either\",\n",
    "                    \"else\",\n",
    "                    \"elsewhere\",\n",
    "                    \"enough\",\n",
    "                    \"entirely\",\n",
    "                    \"especially\",\n",
    "                    \"et\",\n",
    "                    \"etc\",\n",
    "                    \"even\",\n",
    "                    \"ever\",\n",
    "                    \"every\",\n",
    "                    \"everybody\",\n",
    "                    \"everyone\",\n",
    "                    \"everything\",\n",
    "                    \"everywhere\",\n",
    "                    \"ex\",\n",
    "                    \"exactly\",\n",
    "                    \"example\",\n",
    "                    \"except\",\n",
    "                    \"f\",\n",
    "                    \"far\",\n",
    "                    \"few\",\n",
    "                    \"fifth\",\n",
    "                    \"first\",\n",
    "                    \"five\",\n",
    "                    \"followed\",\n",
    "                    \"following\",\n",
    "                    \"follows\",\n",
    "                    \"for\",\n",
    "                    \"former\",\n",
    "                    \"formerly\",\n",
    "                    \"forth\",\n",
    "                    \"four\",\n",
    "                    \"from\",\n",
    "                    \"further\",\n",
    "                    \"furthermore\",\n",
    "                    \"g\",\n",
    "                    \"get\",\n",
    "                    \"gets\",\n",
    "                    \"getting\",\n",
    "                    \"give\",\n",
    "                    \"given\",\n",
    "                    \"gives\",\n",
    "                    \"go\",\n",
    "                    \"goes\",\n",
    "                    \"going\",\n",
    "                    \"gone\",\n",
    "                    \"got\",\n",
    "                    \"gotten\",\n",
    "                    \"greetings\",\n",
    "                    \"h\",\n",
    "                    \"had\",\n",
    "                    \"happens\",\n",
    "                    \"hardly\",\n",
    "                    \"has\",\n",
    "                    \"have\",\n",
    "                    \"having\",\n",
    "                    \"he\",\n",
    "                    \"hello\",\n",
    "                    \"help\",\n",
    "                    \"hence\",\n",
    "                    \"her\",\n",
    "                    \"here\",\n",
    "                    \"hereafter\",\n",
    "                    \"hereby\",\n",
    "                    \"herein\",\n",
    "                    \"hereupon\",\n",
    "                    \"hers\",\n",
    "                    \"herself\",\n",
    "                    \"hi\",\n",
    "                    \"him\",\n",
    "                    \"himself\",\n",
    "                    \"his\",\n",
    "                    \"hither\",\n",
    "                    \"hopefully\",\n",
    "                    \"how\",\n",
    "                    \"howbeit\",\n",
    "                    \"however\",\n",
    "                    \"i\",\n",
    "                    \"ie\",\n",
    "                    \"if\",\n",
    "                    \"ignored\",\n",
    "                    \"immediate\",\n",
    "                    \"in\",\n",
    "                    \"inasmuch\",\n",
    "                    \"inc\",\n",
    "                    \"indeed\",\n",
    "                    \"indicate\",\n",
    "                    \"indicated\",\n",
    "                    \"indicates\",\n",
    "                    \"inner\",\n",
    "                    \"insofar\",\n",
    "                    \"instead\",\n",
    "                    \"into\",\n",
    "                    \"inward\",\n",
    "                    \"is\",\n",
    "                    \"it\",\n",
    "                    \"its\",\n",
    "                    \"itself\",\n",
    "                    \"j\",\n",
    "                    \"just\",\n",
    "                    \"k\",\n",
    "                    \"km\",\n",
    "                    \"keep\",\n",
    "                    \"keeps\",\n",
    "                    \"kept\",\n",
    "                    \"know\",\n",
    "                    \"knows\",\n",
    "                    \"known\",\n",
    "                    \"l\",\n",
    "                    \"last\",\n",
    "                    \"lately\",\n",
    "                    \"later\",\n",
    "                    \"latter\",\n",
    "                    \"latterly\",\n",
    "                    \"least\",\n",
    "                    \"less\",\n",
    "                    \"lest\",\n",
    "                    \"let\",\n",
    "                    \"like\",\n",
    "                    \"liked\",\n",
    "                    \"likely\",\n",
    "                    \"little\",\n",
    "                    \"look\",\n",
    "                    \"looking\",\n",
    "                    \"looks\",\n",
    "                    \"ltd\",\n",
    "                    \"m\",\n",
    "                    \"mainly\",\n",
    "                    \"many\",\n",
    "                    \"may\",\n",
    "                    \"maybe\",\n",
    "                    \"me\",\n",
    "                    \"ms\",\n",
    "                    \"mean\",\n",
    "                    \"meanwhile\",\n",
    "                    \"merely\",\n",
    "                    \"might\",\n",
    "                    \"more\",\n",
    "                    \"moreover\",\n",
    "                    \"most\",\n",
    "                    \"mostly\",\n",
    "                    \"much\",\n",
    "                    \"must\",\n",
    "                    \"my\",\n",
    "                    \"myself\",\n",
    "                    \"n\",\n",
    "                    \"name\",\n",
    "                    \"namely\",\n",
    "                    \"nd\",\n",
    "                    \"near\",\n",
    "                    \"nearly\",\n",
    "                    \"necessary\",\n",
    "                    \"need\",\n",
    "                    \"needs\",\n",
    "                    \"neither\",\n",
    "                    \"never\",\n",
    "                    \"nevertheless\",\n",
    "                    \"new\",\n",
    "                    \"next\",\n",
    "                    \"nine\",\n",
    "                    \"no\",\n",
    "                    \"nobody\",\n",
    "                    \"non\",\n",
    "                    \"none\",\n",
    "                    \"noone\",\n",
    "                    \"nor\",\n",
    "                    \"normally\",\n",
    "                    \"not\",\n",
    "                    \"nothing\",\n",
    "                    \"novel\",\n",
    "                    \"now\",\n",
    "                    \"nowhere\",\n",
    "                    \"o\",\n",
    "                    \"obviously\",\n",
    "                    \"of\",\n",
    "                    \"off\",\n",
    "                    \"often\",\n",
    "                    \"oh\",\n",
    "                    \"ok\",\n",
    "                    \"okay\",\n",
    "                    \"old\",\n",
    "                    \"on\",\n",
    "                    \"once\",\n",
    "                    \"one\",\n",
    "                    \"ones\",\n",
    "                    \"only\",\n",
    "                    \"onto\",\n",
    "                    \"or\",\n",
    "                    \"other\",\n",
    "                    \"others\",\n",
    "                    \"otherwise\",\n",
    "                    \"ought\",\n",
    "                    \"our\",\n",
    "                    \"ours\",\n",
    "                    \"ourselves\",\n",
    "                    \"out\",\n",
    "                    \"outside\",\n",
    "                    \"over\",\n",
    "                    \"overall\",\n",
    "                    \"own\",\n",
    "                    \"p\",\n",
    "                    \"particular\",\n",
    "                    \"particularly\",\n",
    "                    \"per\",\n",
    "                    \"perhaps\",\n",
    "                    \"placed\",\n",
    "                    \"please\",\n",
    "                    \"plus\",\n",
    "                    \"possible\",\n",
    "                    \"presumably\",\n",
    "                    \"probably\",\n",
    "                    \"provides\",\n",
    "                    \"q\",\n",
    "                    \"que\",\n",
    "                    \"quite\",\n",
    "                    \"qv\",\n",
    "                    \"r\",\n",
    "                    \"rather\",\n",
    "                    \"rd\",\n",
    "                    \"re\",\n",
    "                    \"really\",\n",
    "                    \"reasonably\",\n",
    "                    \"regarding\",\n",
    "                    \"regardless\",\n",
    "                    \"regards\",\n",
    "                    \"relatively\",\n",
    "                    \"respectively\",\n",
    "                    \"right\",\n",
    "                    \"s\",\n",
    "                    \"said\",\n",
    "                    \"same\",\n",
    "                    \"saw\",\n",
    "                    \"say\",\n",
    "                    \"saying\",\n",
    "                    \"says\",\n",
    "                    \"second\",\n",
    "                    \"secondly\",\n",
    "                    \"see\",\n",
    "                    \"seeing\",\n",
    "                    \"seem\",\n",
    "                    \"seemed\",\n",
    "                    \"seeming\",\n",
    "                    \"seems\",\n",
    "                    \"seen\",\n",
    "                    \"self\",\n",
    "                    \"selves\",\n",
    "                    \"sensible\",\n",
    "                    \"sent\",\n",
    "                    \"serious\",\n",
    "                    \"seriously\",\n",
    "                    \"seven\",\n",
    "                    \"several\",\n",
    "                    \"shall\",\n",
    "                    \"she\",\n",
    "                    \"should\",\n",
    "                    \"since\",\n",
    "                    \"six\",\n",
    "                    \"so\",\n",
    "                    \"some\",\n",
    "                    \"somebody\",\n",
    "                    \"somehow\",\n",
    "                    \"someone\",\n",
    "                    \"something\",\n",
    "                    \"sometime\",\n",
    "                    \"sometimes\",\n",
    "                    \"somewhat\",\n",
    "                    \"somewhere\",\n",
    "                    \"soon\",\n",
    "                    \"sorry\",\n",
    "                    \"specified\",\n",
    "                    \"specify\",\n",
    "                    \"specifying\",\n",
    "                    \"still\",\n",
    "                    \"sub\",\n",
    "                    \"such\",\n",
    "                    \"sup\",\n",
    "                    \"sure\",\n",
    "                    \"t\",\n",
    "                    \"take\",\n",
    "                    \"taken\",\n",
    "                    \"tell\",\n",
    "                    \"tends\",\n",
    "                    \"th\",\n",
    "                    \"than\",\n",
    "                    \"thank\",\n",
    "                    \"thanks\",\n",
    "                    \"thanx\",\n",
    "                    \"that\",\n",
    "                    \"thats\",\n",
    "                    \"the\",\n",
    "                    \"their\",\n",
    "                    \"theirs\",\n",
    "                    \"them\",\n",
    "                    \"themselves\",\n",
    "                    \"then\",\n",
    "                    \"thence\",\n",
    "                    \"there\",\n",
    "                    \"thereafter\",\n",
    "                    \"thereby\",\n",
    "                    \"therefore\",\n",
    "                    \"therein\",\n",
    "                    \"theres\",\n",
    "                    \"thereupon\",\n",
    "                    \"these\",\n",
    "                    \"they\",\n",
    "                    \"think\",\n",
    "                    \"third\",\n",
    "                    \"this\",\n",
    "                    \"thorough\",\n",
    "                    \"thoroughly\",\n",
    "                    \"those\",\n",
    "                    \"though\",\n",
    "                    \"three\",\n",
    "                    \"through\",\n",
    "                    \"throughout\",\n",
    "                    \"thru\",\n",
    "                    \"thus\",\n",
    "                    \"to\",\n",
    "                    \"together\",\n",
    "                    \"too\",\n",
    "                    \"took\",\n",
    "                    \"toward\",\n",
    "                    \"towards\",\n",
    "                    \"tried\",\n",
    "                    \"tries\",\n",
    "                    \"truly\",\n",
    "                    \"try\",\n",
    "                    \"trying\",\n",
    "                    \"twice\",\n",
    "                    \"two\",\n",
    "                    \"u\",\n",
    "                    \"un\",\n",
    "                    \"under\",\n",
    "                    \"unfortunately\",\n",
    "                    \"unless\",\n",
    "                    \"unlikely\",\n",
    "                    \"until\",\n",
    "                    \"unto\",\n",
    "                    \"up\",\n",
    "                    \"upon\",\n",
    "                    \"us\",\n",
    "                    \"use\",\n",
    "                    \"used\",\n",
    "                    \"useful\",\n",
    "                    \"uses\",\n",
    "                    \"using\",\n",
    "                    \"usually\",\n",
    "                    \"uucp\",\n",
    "                    \"v\",\n",
    "                    \"value\",\n",
    "                    \"various\",\n",
    "                    \"very\",\n",
    "                    \"via\",\n",
    "                    \"viz\",\n",
    "                    \"vs\",\n",
    "                    \"w\",\n",
    "                    \"want\",\n",
    "                    \"wants\",\n",
    "                    \"was\",\n",
    "                    \"way\",\n",
    "                    \"we\",\n",
    "                    \"welcome\",\n",
    "                    \"well\",\n",
    "                    \"went\",\n",
    "                    \"were\",\n",
    "                    \"what\",\n",
    "                    \"whatever\",\n",
    "                    \"when\",\n",
    "                    \"whence\",\n",
    "                    \"whenever\",\n",
    "                    \"where\",\n",
    "                    \"whereafter\",\n",
    "                    \"whereas\",\n",
    "                    \"whereby\",\n",
    "                    \"wherein\",\n",
    "                    \"whereupon\",\n",
    "                    \"wherever\",\n",
    "                    \"whether\",\n",
    "                    \"which\",\n",
    "                    \"while\",\n",
    "                    \"whither\",\n",
    "                    \"who\",\n",
    "                    \"whoever\",\n",
    "                    \"whole\",\n",
    "                    \"whom\",\n",
    "                    \"whose\",\n",
    "                    \"why\",\n",
    "                    \"will\",\n",
    "                    \"willing\",\n",
    "                    \"wish\",\n",
    "                    \"with\",\n",
    "                    \"within\",\n",
    "                    \"without\",\n",
    "                    \"wonder\",\n",
    "                    \"would\",\n",
    "                    \"would\",\n",
    "                    \"x\",\n",
    "                    \"y\",\n",
    "                    \"yes\",\n",
    "                    \"yet\",\n",
    "                    \"you\",\n",
    "                    \"your\",\n",
    "                    \"yours\",\n",
    "                    \"yourself\",\n",
    "                    \"yourselves\",\n",
    "                    \"z\",\n",
    "                    \"zero\",\n",
    "                    \"project\",\n",
    "                    \"overall project summary\",\n",
    "                    \"abstract\",\n",
    "                    \"project description\",\n",
    "                    \"description\",\n",
    "                    \"summary\",\n",
    "                    \"description provided by applicant:\",\n",
    "                    \"overall\",\n",
    "                    \"applicant\",\n",
    "                    \"purpose\",\n",
    "                    \"summaryabstract\",\n",
    "                    \"descriptionabstract\",\n",
    "                    \"made\",\n",
    "                    \"highly\",\n",
    "                    \"research\",\n",
    "                    \"important\",\n",
    "                    \"study\",\n",
    "                    \"examine\",\n",
    "                    \"questions\",\n",
    "                    \"range\",\n",
    "                    \"funding\",\n",
    "                    \"funded\",\n",
    "                    \"program\",\n",
    "                    \"large\",\n",
    "                    \"based\",\n",
    "                    \"areas\",\n",
    "                    \"high\",\n",
    "                    \"field\",\n",
    "                    \"show\",\n",
    "                    \"provide\",\n",
    "                    \"successful\",\n",
    "                    \"application\",\n",
    "                    \"proposal\",\n",
    "                    \"lead\",\n",
    "                    \"approach\",\n",
    "                    \"closely\",\n",
    "                    \"knowledge\",\n",
    "                    \"continued\",\n",
    "                    \"support\",\n",
    "                    \"receive\",\n",
    "                    \"method\",\n",
    "                    \"david\",\n",
    "                    \"greatly\",\n",
    "                    \"seminar\",\n",
    "                    \"face\",\n",
    "                    \"shown\",\n",
    "                    \"needed\",\n",
    "                    \"area\",\n",
    "                    \"academic\",\n",
    "                    \"worldwide\",\n",
    "                    \"proposed\",\n",
    "                    \"great\",\n",
    "                    \"goal\",\n",
    "                    \"focus\",\n",
    "                    \"specific\",\n",
    "                    \"remains\",\n",
    "                    \"essential\",\n",
    "                    \"small\",\n",
    "                    \"big\",\n",
    "                    \"large\",\n",
    "                    \"recently\",\n",
    "                    \"supports\",\n",
    "                    \"successfully\",\n",
    "                    \"require\",\n",
    "                    \"students\",\n",
    "                    \"training\",\n",
    "                    \"support\",\n",
    "                    \"program\",\n",
    "                    'https',\n",
    "                    'doi',\n",
    "                    'org',\n",
    "                    'www',\n",
    "                    \"http\",\n",
    "                    \"identify\",\n",
    "                    \"function\",\n",
    "                    \"call\",\n",
    "                    \"measure\",\n",
    "                    \"understand\",\n",
    "                    \"china\",\n",
    "                    \"uk\",\n",
    "                    \"easet\",\n",
    "                    \"north\",\n",
    "                    \"africa\",\n",
    "                    \"india\",\n",
    "                    \"west\",\n",
    "                    \"south\",\n",
    "                    \"january\",\n",
    "                    \"february\",\n",
    "                    \"march\",\n",
    "                    \"april\",\n",
    "                    \"may\",\n",
    "                    \"june\",\n",
    "                    \"july\",\n",
    "                    \"august\",\n",
    "                    \"september\",\n",
    "                    \"october\",\n",
    "                    \"november\",\n",
    "                    \"december\",\n",
    "                    \"measure\",\n",
    "                    \"call\",\n",
    "                    \"local\",\n",
    "                    \"investigate\",\n",
    "                    \"english\",\n",
    "                    \"chinese\",\n",
    "                    \"apply\",\n",
    "                    \"cover\",\n",
    "                    \"programme\",\n",
    "                    \"propose\",\n",
    "                    \"award\",\n",
    "                    \"enable\",\n",
    "                    \"participation\",\n",
    "                    \"decide\",\n",
    "                    \"pursue\",\n",
    "                    \"work\",\n",
    "                    \"survey\",\n",
    "                    \"underly\",\n",
    "                    \"objective\",\n",
    "                    \"contact\",\n",
    "                    \"progress\",\n",
    "                    \"due\",\n",
    "                    \"extend\",\n",
    "                    \"induce\",\n",
    "                    \"test\",\n",
    "                    \"count\",\n",
    "                    \"detected\",\n",
    "                    \"recent\", \"research\", 'project', \"study\", \"cell\", \"model\",\n",
    "                    \"th\", \"nd\", \"rd\",\n",
    "                    \"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"lt\",\n",
    "                    \"relate\", \"data\", \"significant\", \"develop\", \"make\", \"base\"\n",
    "                    \"oxford\", \"programme\", \"japanese\", \"durham\", \"aim\", \"include\", \n",
    "                    \"british\", \"london\", \"sector\", \"affect\", \"sheet\", \"aspect\", \"main\", 'increase', 'input', 'vast', 'amount'\n",
    "                    \"detail\", 'future', 'dataset', 'unique', 'linkage', \"predict\", 'channel', 'experience', \n",
    "                    \"bed\", \"poorly\", 'estimate'])\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85165ce0-3908-4c9f-9c8d-01657487680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_stop_words(sw = stop_words):\n",
    "    port = PorterStemmer()\n",
    "    return set([port.stem(token) for token in stop_words])\n",
    "\n",
    "stemmed_stop_words = stemmed_stop_words()\n",
    "\n",
    "def save_word_list(path, word_list):\n",
    "    # Save high- and low- freqency words\n",
    "    with open(path, 'w') as filehandle:\n",
    "        for listitem in word_list:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for txt in tqdm(sentences):\n",
    "        txt = re.sub('&[A-Za-z]+;[A-Za-z]+;[A-Za-z]+;', ' ', txt)\n",
    "        txt = re.sub('&[A-Za-z]+;[A-Za-z]+;', ' ', txt)\n",
    "        txt = re.sub('&[A-Za-z]+;', ' ', txt)\n",
    "        txt = re.sub('[^A-Za-z\\s]', ' ', txt)\n",
    "        txt = gensim.utils.simple_preprocess(str(txt), deacc=True) \n",
    "        yield(txt)  \n",
    "        \n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, stemmed_stop_words = stemmed_stop_words):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    print(\"Starting to remove stopwords...\")\n",
    "    st = t.time()\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    print(\"Stopwords removed, it takes {} seconds to run.\".format(t.time() - st))\n",
    "    print(\"Starting to create bigrams and trigrams...\")\n",
    "    st = t.time()\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    print(\"Bigrams and trigrams created, it takes {} seconds to run.\".format(t.time() - st))\n",
    "    print(\"Starting to stem words...\")\n",
    "    st = t.time()\n",
    "    texts_out = []\n",
    "    for sent in tqdm(texts):\n",
    "#         doc = (\" \".join(sent))\n",
    "        port = PorterStemmer()\n",
    "        texts_out.append([port.stem(token) for token in sent])\n",
    "    print(\"Words lemmatized, it takes {} seconds to run.\".format(t.time() - st))\n",
    "    print(\"Starting to remove stop words after stemming...\")\n",
    "    # remove stopwords once more after stemming\n",
    "    texts_out_stemmed_noStopWords = []\n",
    "    for doc_words in tqdm(texts_out):\n",
    "#         texts_out = [[word for word in doc_words if word not in stemmed_stop_words] for doc_words in texts_out]   \n",
    "        texts_out_stemmed_noStopWords.append([word for word in doc_words if word not in stemmed_stop_words])\n",
    "    print(\"All finished!\")\n",
    "    return texts_out_stemmed_noStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d763b3-e1c1-4afa-b2f1-66fdc758cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_text(data, numWords=5):\n",
    "    col = data['data_ready'].apply(lambda s: True if len(s) > numWords else False).tolist()\n",
    "    data = data[data.index.isin([i for i, x in enumerate(col) if x])]\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    return data\n",
    "\n",
    "def read_word_list(path = '../Data/unimportant_stemmed_words_ALL.txt'):\n",
    "    with open(path, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    return [x.replace(\"\\n\", '').strip() for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123a87d5-019a-4f91-9ef2-ea5f0f477aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Cleaned_Data.csv\")\n",
    "data = df.abstract.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f2251f3-4e42-4e8f-9a6d-353542854606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 145787/145787 [02:05<00:00, 1160.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e905d9a-8d49-4c0d-b1de-f9b78d3d9e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145787"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8666a6fa-95aa-4ad0-a783-89d4ed0e4392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 20s, sys: 5.27 s, total: 4min 25s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87be512-f659-441c-ad68-8bae2d34a4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to remove stopwords...\n",
      "Stopwords removed, it takes 79.84433794021606 seconds to run.\n",
      "Starting to create bigrams and trigrams...\n",
      "Bigrams and trigrams created, it takes 64.1935670375824 seconds to run.\n",
      "Starting to stem words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 145787/145787 [07:24<00:00, 327.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words lemmatized, it takes 444.49759316444397 seconds to run.\n",
      "Starting to remove stop words after stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 145787/145787 [00:08<00:00, 16854.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished!\n"
     ]
    }
   ],
   "source": [
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd0577db-2545-473b-9c68-3d3b5477ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"data_ready\"] = data_ready\n",
    "df = remove_short_text(df, 5)\n",
    "data_ready = df.data_ready.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a9339d2-d0f8-403c-bf6d-d95643af0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Data/Cleaned_Data2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea23fe26-8c56-481c-8f5f-10b774d83df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ready = [\" \".join(wl) for wl in data_ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2a8da58-9b5c-4f02-a6ce-e0f7f609c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word_list(path = '../Data/data_ready/Cleaned_Documents.txt', word_list = doc_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c81c74f-74bc-4d8a-abc1-4ae6ad56835b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145712"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd0cc5d-b627-4d98-a4b5-0dac5b7ecfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24944683"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([word for wl in data_ready for word in wl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92ae251b-0a53-4f2b-828f-cbd932f7464d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210655"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unique_words = list(set([word for wl in data_ready for word in wl]))\n",
    "len(all_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f63d2-a803-4295-804d-7f1f2aa9ec05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
