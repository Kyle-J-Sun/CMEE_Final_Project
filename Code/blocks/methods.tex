\section{Methods}

\subsection{Experimental Data}

Data was collected from the websites of UKRI, NIH, NSF and ERC. After removing projects of which abstracts were unavailable, data from 145,787 funded projects awarded between 2015 and 2021 were collected, including 27,035 project abstracts from the UKRI funding agency collected by the GtR API program \cite{UKRI}, 41,796 abstracts from the NSF database of funded projects \cite{NSF}, 71,008 abstracts from NIH collected using RePORTER database and finally 5,948 abstracts from ERC collected directly using its website database \cite{NIH, ERC}.

To obtain more accurate results, project abstracts need to be preprocessed in ways that are required by any text mining technique. The first step was to remove all HTML character references (e.g. \&amp; or \&lt;), which is a way to display special characters on a website page since some of the abstracts in my dataset are scraped from websites. Then, I removed all punctuations, non-English words and English stop words such as ``You", ``I", ``necessary" and ``on" \textit{etc.} Some of the stop words are from the NLTK package in Python and some are identified manually (see Support Information). In addition, to improve the accuracy of the LDA model, bigram and trigram models were utilised to identify two words or three words that have a high probability to occur together \cite{ahmed2009}. For example, if the word set \{chimeric, antigen, receptor\} could have a higher probability to show up together in the genetic- or protein-related topics than physics-related topics, then the trigram will treat them as a single word with an underscore (e.g. chimeric\_antigen\_receptor).

Also, these co-occurring words are helpful for describing topics produced by the LDA model, even though a few co-occurring words are not ideal such as `erc\_start' \cite{sseutm}. Afterwards, all remaining words were stemmed, meaning that all inflected words were cut back to a common root \cite{rbcbaestmro32000a}. For instance, all words like ``damage", ``damaged" and ``damaging" were transformed into ``damag". Next, all high-frequency (occur in more than 80\% of abstracts) and low-frequency (occur in less than 0.1\% of abstracts) words were removed. Finally, 7060 unique words are left to fit the topic models.

The final dataset consists of project title, project abstracts, funding amount of projects, the project duration of projects. Because four funding agencies are not founded in the same country, meaning the currency are distinct from each other. For consistency, the currency of funding amount in four agencies is converted to U.S. Dollars (\$) with real-time exchange rates using the Fixer API program \cite{fixer}.

\subsection{Latent Dirichlet Allocation}

Latent Dirichlet Allocation (LDA) is one of the most popular models in topic analysis, which is an unsupervised machine learning technique. First, I introduce $D$ as the number of documents, $V$ as the number of words in corpus and $K$ as the number of topics. LDA treats $D$ documents in the dataset as a probability distribution of $K$ topics. LDA is used to infer $K$ latent topics that can summarise $V$ words in text documents \cite{sseutm}. From this perspective, the LDA model can be also regarded as a dimensionality reduction tool, transforming $V$ words into $K$ clusters (i.e. topics). Two matrices are inferred in the LDA model for obtaining topic distributions for $D$ documents \cite{LDA}. The first matrix is a document-topic matrix with $D$ rows of documents and $K$ columns of topics, the values in the matrix are probabilities of each document belonging to each topic. Hence, the sum of probabilities of $K$ topics for each document should be one. Similarly, the topic-word matrix enables $K$ topics to have words with different weights representing how important each word is. For instance, if the high weights are generated by the LDA model for some words such as `climate', `emission', `change', `environment' and `temperature', then this might be a topic related to ``Climate Change" or ``Environment Protection".

In this project, the LDA model is implemented by collapsed Gibbs sampling (see Support Information section). The collapsed Gibbs sampling method has the following basic process: (1) Select a document and randomly assign topics for each word in the document. (2) Repeat the same process for every document in the corpus and then summarise local and global (i.e. for all documents) statistics for each topic. For example, if a document has words such as `epilepsy', `dynamic', `bayesian', `EEG' and `model', one Ô¨Årst assign topics for each word in the document and summarises the number of words each topic included in the document (local statistics) and the number of times for each word grouped into each topic in all documents (global statistics). (3) Topics are reassigned randomly again and the probability of a new assignment is estimated. (4) The same process is iterated throughout the words and documents. The probability of a new assignment can be estimated by \cite{gibbs_lda}:

\begin{equation}
  P = \frac{n_{ik} + \alpha}{N_i - 1 + K\alpha} \frac{m_{t,k} + \beta}{\sum_{w\in V}m_{t,k} + V\beta}
\end{equation}

where $n_{ik}$ is the number of current assignments to topic $k$ in doc $i$, $N_i$ is the number of words in document $i$, $\alpha$ is a hyperparameter that controls the prior distribution over topic weights in each document, $K$ represents the number of topics, $m_{t,k}$ means the number of assignments of given word $t$ to topic $k$, $V$ is the size of vocabulary and $\beta$ is a hyperparameter for the prior distribution over word weights in each topic. Tuning two hyperparameters also affect inferences of the LDA model. Higher $\alpha$ can make document preference ``smoother" over topics, enabling the model to generate more topics with higher probability; higher $\beta$ also enables more terms to be important for topic description and vice versa, i.e. topics preferences are ``smoother" over their keywords. Therefore, for simplicity, my research used default values of $\alpha$ and $\beta$ in Gensim \cite{gensim}, which both are one over the number of topics. Additionally, because collapsed Gibbs sampling is based on Markov Chain Monte Carlo (MCMC) algorithm, leading to the stochastic output when the same LDA model with fixed parameters is run for multiple times, explaining why I need to conduct a $stability$ test for multiple topic models in the following section \cite{chuang2015, Wang2019}.

\subsection{Coherence Measurement}

The coherence measurement is an approach to examine how the words in the same topic support each other \cite{aletras2013}. Many metrics have been developed to measure how coherent a set of words in each topic such as $C_{uci}$\cite{lau2014}, $U_{mass}$ \cite{Mimno2011}, $C_{npmi}$\cite{bouma2009}, $C_v$ \cite{michael2015} and $C_p$ \cite{fitelson2003}. In this project, the $C_v$ measure is applied since it has been tested that it outperformed all other metrics in coherence measurement \cite{michael2015}. $C_v$ is based on a sliding window \cite{newman2010}, which uses one-set segmentation (i.e. any single word within a set is paired with every other single word for comparison) between topic words as well as an indirect confirmation measure \cite{aletras2013} to calculate normalized pointwise mutual information (NPMI) for each word pair \cite{michael2015}. Afterwards, the cosine similarity for each NPMI of word pairs is calculated \cite{newman2010}. The equation of NPMI can be expressed as:

\begin{equation}
  NPMI(w_i, w_j)^{\gamma} = \left(\frac{PMI(w_i, w_j)}{-log(P(w_i, w_j) + \epsilon)}\right)^{\gamma}
\end{equation}

where

\begin{equation}
PMI(w_i, w_j) = \log\frac{P(w_i, w_j) + \epsilon}{P(w_i) \cdot P(w_j)}
\end{equation}

where $P(w_i, w_j)$ is the probability of words $w_i$ and $w_j$ co-occur, $\epsilon$ is a parameter that can avoid the logarithm of zero, $\gamma$ is a parameter that can give more weights on higher NPMI values as $\gamma$ increases.

\subsection{Model Selection}

To obtain an efficient topic model, I first run topic models with a given number of topics $k$ with 10,000 iterations \cite{mariana2017}, where each model $M^k$ are repeated by 10 times and each repetition has a single $C_v$ value. Therefore, I obtained 10 different coherence values for each model $M^k$. Mathematically speaking, each model $M^k$ has a vector of coherence values, expressed as:

\begin{equation}
  \begin{bmatrix} Coh^{k}_{1} & Coh^{k}_{2} & \dots & Coh^{k}_{n} \end{bmatrix}
\end{equation}

where $n = 10$ and $Coh^{k}_{i}$ means the $i$-th coherence measurement (i.e. $C_v$ value) for a k-topic model (i.e. the topic model with $k$ topics).

In this project, I tested 32 topic models whose the $k$ starts from 2 to 64 with increment 2 (i.e. $k \in \{ 2,4,6,...,64 \} $). Therefore, I gained 32 vectors of coherence values, expressed as a ($p \times n$) coherence matrix:

\begin{equation}
  \begin{bmatrix}
  Coh^{k = 2}_{11} & Coh^{k = 2}_{21} & \dots & Coh^{k = 2}_{n1} \\
  Coh^{k = 4}_{12} & Coh^{k = 4}_{22} & \dots & Coh^{k = 4}_{n2} \\
  \vdots & \vdots & \ddots & \vdots \\
  Coh^{k = 64}_{1p} & Coh^{k = 64}_{2p} & \dots & Coh^{k = 64}_{np} \\
  \end{bmatrix}
\end{equation}

where $n$ is the number of coherence values in each k-topic model, $p$ is the number of k-topic models.

Afterwards, I computed the median $C_v$ ($Cv_{med}^{k}$) values for each row in the coherence matrix, transforming the coherence matrix into a $(p \times 1)$ vector, shown as follows:

\begin{equation}
  \begin{bmatrix}
  Cv_{med}^{k = 2} \\
  Cv_{med}^{k = 4} \\
  \vdots \\
  Cv_{med}^{k = 64} \\
  \end{bmatrix}
\end{equation}

where each element in the vector represents the median $C_v$ value for a k-topic model $M^k$. Then I choose a subset of $M^k$s with relatively high $Cv_{med}^{k}$ and make a comparison with each other using $stability$ and mean $similarity$ approach (see Support Information) to find the final topic model.

\subsection{Prediction of Funding Amount}

To find out whether the topic distributions I obtained using the LDA model have prediction power or improved prediction compared to the project duration, I used a supervised machine-learning tool to forecast the discrete funding amount of projects. First, I divided the four agencies into two groups: the NIH group (the health-specialist agency) and all the other agencies including NSF, ERC and UKRI. Two targets were predicted, which are the total funding amount for each project and the funding amount per day, called daily funding amount in this project, which is calculated by:

\begin{equation}
  A_{daily} = \frac{A_{total}}{Duration}
\end{equation}

where $A_{daily}$ is the daily funding amount, $A_{total}$ is the total funding amount, $Duration$ is the project duration in the unit of days.

Then, I treat the prediction as a multiclassification problem. Thus, all funding amounts are discretized to three different classes (i.e. low-amount zone, medium-amount zone and high-amount zone). For the multiclassification problem, I utilised a popular gradient boosting framework known as Light Gradient Boosting Machine (LightGBM) \cite{Ke2017}. LightGBM is based on the gradient boosting decision tree (GBDT), which is a prevalent machine-learning algorithm \cite{jerome2001} that can be used to solve multiclassification problems \cite{li2010} and learning to rank \cite{burges2010}. However, different from GBDT, LightGBM leverages approaches of Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). GOSS method retains only data instances with larger gradients. It can provide more information gain, obtaining more accurate outcomes. Meanwhile, the EFB approach can optimize the efficiency of feature selection, enabling the LightGBM algorithm to handle data with less computational complexity \cite{Ke2017}. The LightGBM has been proved that it can outperform XGBoost and SGB algorithms concerning the efficiency of computation of memory consumption \cite{Ke2017}. Therefore, the prediction was done by solely LightGBM.

I created separate LightGBM models according to datasets of two groups (NIH and all other agencies). For each group, I built three models with distinct predicting variables. The first one is the LightGBM model with all topic distributions as predicting variables (defined as $M_{topic}$), the second model is created with only project duration as a feature, defined as $M_{duration}$. The third model has all topic distributions and project duration as predicting variables, defined as $M_{all}$. All predicting variables used for the three models are normalised by using the ``StandardScaler" function in the ``sklearn" package in Python \cite{sklearn}. Finally, these two groups comprised 70,842 and 74,141 observations respectively, with a balanced number of classes and six models in total (i.e. three models for each group). 70\% of observations are used as the training set, the remaining observations are used as the test set for prediction in both two groups.

There are three steps to make a prediction: (1) I fitted all models described above by using normalised predictors. (2) I chose parameters for each model, where the learning rate is 0.02, boosting type is GBDT (i.e Gradient Boosting Decision Tree), the maximum depth of the tree is 80, the number of leaves is 200 and the number of iterations is 150. The parameters of all models are the same. (3) The probabilities of three classes for each data point were gained. I then chose a class with the highest probability as a label for each observation. The accuracy of each model was calculated by the expression:

\begin{equation}
  Accuracy = \frac{tp}{tp + fp}
\end{equation}

where $tp$ is the number of true positives and $fp$ is the number of false positives, indicating the ability of the model not to assign positives to the data that is negative.

\subsection{Computing Tools}

In this project, all scraped data are stored in a MySQL database. ALL analytical techniques are conducted using Python 3, where the ``pymysql" package is used for extracting data from MySQL database, packages ``numpy", ``pandas", ``NLTK", ``pyinflect" and ``sklearn" are used for data preprocessing, the LDA model used in the project is implemented by the ``Gensim" package, the ``pyLDAvis" package is used for model visualisation.

\subsection{Data Availability}

All scripts of data preprocessing, text-mining and machine-learning modelling are available at \url{https://github.com/Kyle-J-Sun/CMEE_Final_Project}
