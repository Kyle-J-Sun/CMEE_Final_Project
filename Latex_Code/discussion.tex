\section{Discussion}

By using combined machine-learning approaches, I found that topic modelling techniques are helpful for topic assignment in these four funding agencies because the LDA model correctly assigned medicine- and health-related projects into NIH agency. Also, I looked into the part of project abstracts in the other three agencies and the topics also can be found interpretably. Moreover, the LightGBM might be useful for the prediction of funding amount since the accuracy of models $M_{all}$ can beyond 0.6 as I used both the topic distributions and project duration as prediction variables, even though the accuracy of $M_{all}$ is poorer as predicting daily funding amount than total funding amount in the group of all other agencies. However, the accuracy is below 0.5 if I use only topic distributions as predictors to forecast either total or daily funding amount, indicating that the topic distributions generated from the LDA model don't show the prediction power.

From heatmap analysis on topics, a significant topic bias can be seen in four prevalent academic agencies. Except for NIH, which is a health-focused funding agency, the funding of the other three agencies is occupied by topics related to Computer Systems, Commercial Applications, Material Science and Physics. In addition, protein chemistry and microbiology are also prevalent topics in the ERC agency. Thus, NSF, ERC and UKRI are biased by STEM-related topics. The topic ``Climate Change, Environment" are ignored significantly by all four popular funding agencies. Only two dominated topics in NIH (``Computer System and Commercial Application") and in all other agencies (``Cancer Treatment and Immunology") are equally distributed in almost 50 intervals of the daily funding amount.

However, a positive or negative relationship with daily funding amount also can be seen in two groups. For example, ``Cancer Treatment and Immunology", dominating the group of all other agencies, increases its occurrence probability with funding amount in the NIH group, the occurrence probability of ``Computer System \& Commercial Application", dominating the NIH group, also rises with daily funding amount in the group of all other agencies. This suggests that a project that is mainly relevant to ``Cancer Treatment and Immunology" and ``Computer System and Commercial Application" (e.g. Medical Engineering) will have a higher probability of obtaining a higher daily funding amount in any of four agencies. Similarly, a project will be more likely to be funded with a high amount in the group of all other agencies if it includes a large portion of ``Computer System \& Commercial Application" with ``Cancer Treatment and Immunology" or ``Protein Chemistry and Microbiology" or ``Brain Science". (e.g. possibly Computational Biology or Computational Neuroscience).

So far, the quantitative analysis combining the LDA model and LightGBM is first introduced in this field. Thus, there are some limitations to the project and possible solutions to be completed in the future. In this project, the dataset of four agencies is obtained in different ways, resulting in the difficulty of data collection. Therefore, one can develop a program that can automatically scrape up-to-date data from the websites of the agencies without the breach of data privacy statement, allowing the models used in the project to update regularly.

Moreover, I used an 8-topic model by comparing models that are run multiple times with different $k$. However, because of time and computational restriction, only 32 topic models are included and each model is repeated only 10 times, leading the final topics to the lack of specificity. For example, for the topic ``social policy and public healthcare", I can not give a precise conclusion that the prevalence of the topic is due to ``Social Policy" or because of ``Public Healthcare". The problem can be solved by testing a wider range of $k$ and more repetition times for each model if time and computational power allow.

Second, the parameters of all LightGBM models are all set to be the same on a rule-of-thumb basis. However, a better approach to parameter optimisation could be implemented in order to mitigate the risk of overfitting. For example, the Bayesian hyper-parameter optimization algorithm was utilised by Wang \textit{et al.} to improve the performance of LightGBM \cite{wang2020}.

Next, I used $M_{duration}$ and $M_{all}$ to study the improvement of the model accuracy by topic distributions. However, the only project duration can not always provide sufficient information of the data, leading to poor accuracy. This might be the reason why $M_{duration}$ and $M_{all}$ in the group of all other agencies give worse performance as predicting the daily funding amount than predicting the total funding amount. Therefore, one can add more factors that could affect funding amount to improve the accuracy of the models such as the h-index of PI and the rank of PI's university \textit{etc.}

Then, to assess the performance of the LightGBM models, only the accuracy metric is used (see section 2.5). However, the accuracy (or precision) score can be reasonable only if the cost of false positives is high. Therefore, more approaches can be applied to assess the model performance such as recall, F1 Score and receiver operating characteristic (ROC) Curve. Furthermore, prediction on funding amount has been treated as a multiclassification problem. So, other prediction methods are worthy of consideration in order to acquire more specific outcomes such as regression on funding amount.

Last but not the least, due to the impossible collection of unfunded project data, the relationship between topic distributions and funding failure can not be explored yet. So, if the information of unfunded projects is available, one can use the same approaches in the research to forecast funding success and failure. For example, instead of predicting academic funding amount, a similar method was developed by Mitra \textit{et al.} (2014) for predicting crowdfunding success and failure by penalized logistic regression with phrases produced by unigram, bigram and trigram models as features \cite{mitra2014}.
